name: "Prequential Coding - Emergent Languages"
# Look at debug_continuous.yaml
framework:
  _target_: frameworks.prequential_coding.PrequentialCodingSentenceDecoder
  lr: 0.001
  discrete_z: ${..data.discrete} 
  z_num_attributes: ${..data.num_attributes} 
  z_num_vals: ${..data.num_vals} 
  interval_patience: 15 # 50 -- if too slow, decrease - is number of steps to wait before restarting training
  interval_patience_tol: 0.001
  include_initial_length: True
  model:
    _target_: models.decoders.SentenceDecoder
    vocab_size: ${...data.vocab_size}
    embedding_decoder: # Use same architecture as the sentence embedding encoder, use same padding token, take from huggingface uninitialized
      # to use HuggingFace, write wrapper class around Hugging Face model
      _target_: models.decoders.HuggingFaceEmbeddingDecoder
      emb_dim: 768
      # num_words: ${....data.num_words}
      # repr_dim: ${repr_dim:'${....data.num_words} * ${....data.vocab_size}'} # 40
      # num_layers: 4
      # hidden_dim: 256
      # fixed_repr_std: 0.35 # 1.0
      # dropout: 0.0

data:
  _target_: custom_datasets.debug_data.DebugPrequentialDataModule
  data_dir: "/network/scratch/t/thomas.jiralerspong/kolmogorov/results/to_prequential_code/with_reset_topo=0.234_acc=84"
  min_data_size: 100 # keep same as data_increment, where starting to actually fit models
  val_size: 2000
  data_increment: 100 # number of data points to add each time, can increase if too slow
  batch_size: 100
  num_words: 4 # Use max length across dataset and pad
  vocab_size: 10 # Check tokenizer
  disentanglement: 1
  discrete: False
  z_dim: 768
  num_attributes: null
  num_vals: null

trainer:
  max_epochs: 100000

callbacks: False